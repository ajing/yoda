{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "> yoda wants to simplify the way to run jobs on Google AI platform and organize your model process in a config file.\n",
    "\n",
    "In this session, we will go through a few examples to see how yoda works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import click\n",
    "import yaml\n",
    "import blocks\n",
    "import typing\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import numpy as np\n",
    "from sklearn.metrics import SCORERS\n",
    "\n",
    "from functools import lru_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@click.group()\n",
    "def cli():\n",
    "    pass\n",
    "\n",
    "@cli.command()\n",
    "@click.argument('config', type=click.File('r'))\n",
    "def run(config):\n",
    "    # process\n",
    "    conf_dict = yaml.load(config, Loader=yaml.FullLoader)\n",
    "    data = Data(**conf_dict['data'])\n",
    "    train = Train(**conf_dict['train'])\n",
    "\n",
    "    if \"eval\" in conf_dict or data.eval_path:\n",
    "        eval_type = conf_dict.get(\"eval\")\n",
    "        eval_path = data.eval_path\n",
    "        metrics = conf_dict[\"eval\"].get(\"metrics\").split(\",\")\n",
    "        eval(eval_path, eval_type, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "here\n\n"
    }
   ],
   "source": [
    "from click.testing import CliRunner\n",
    "\n",
    "runner = CliRunner()\n",
    "result = runner.invoke(run, ['../data/configs/config1.yaml'])\n",
    "print(result.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run on local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of a config file `config1.yaml`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "data: \n  input_path: \"../data/iris_data.csv\"\n  eval_path: \"../data/iris_data.csv\"\n  output_path: \"../output/\"\n  features: \"sepal_length,sepal_width,petal_length\"\n  label: species\ntrain:\n  estimator: xgboost.XGBRegressor\n  params:\n    max_depth: 4\n    num_estimator: 50\neval:\n  metric: accuracy\n"
    }
   ],
   "source": [
    "config1 = '../data/configs/config1.yaml'\n",
    "with open(config1) as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run this config file locally by \n",
    "\n",
    "```{shell}\n",
    "yoda run config1.yaml\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following is how yoda process the config file, you can safely ignore this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the file\n",
    "conf_dict = yaml.load(open(config1), Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'data': {'input_path': '../data/iris_data.csv',\n  'eval_path': '../data/iris_data.csv',\n  'output_path': '../output/',\n  'features': 'sepal_length,sepal_width,petal_length',\n  'label': 'species'},\n 'train': {'estimator': 'xgboost.XGBRegressor',\n  'params': {'max_depth': 4, 'num_estimator': 50}},\n 'eval': {'metric': 'accuracy'}}"
     },
     "metadata": {},
     "execution_count": 114
    }
   ],
   "source": [
    "conf_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Data:\n",
    "    def __init__(self, input_path: str, output_path: str, features: str, label: str, **kwargs):\n",
    "        self.input_path = input_path\n",
    "        self.output_path = output_path\n",
    "        self.feature_list = features.split(\",\")\n",
    "        self.label = label\n",
    "        self.eval_path = kwargs.get(\"eval_path\", None)\n",
    "        self.score_path = kwargs.get(\"score_path\", None)\n",
    "\n",
    "    @property\n",
    "    @lru_cache(1)\n",
    "    def df(self):\n",
    "        return blocks.assemble(self.input_path)\n",
    "\n",
    "    @property\n",
    "    def X(self):\n",
    "        return self.df[self.feature_list]\n",
    "\n",
    "    @property\n",
    "    def y(self):\n",
    "        return self.df[self.label]\n",
    "\n",
    "    @property\n",
    "    @lru_cache(1)\n",
    "    def eval_df(self):\n",
    "        if not self.eval_path:\n",
    "            raise Exception(\"Please specify the eval_path\")\n",
    "        return blocks.assemble(self.eval_path)\n",
    "    \n",
    "    @property\n",
    "    def eval_X(self):\n",
    "        return self.eval_df[self.feature_list]\n",
    "\n",
    "    @property\n",
    "    def eval_y(self):\n",
    "        return self.eval_df[self.label]\n",
    "\n",
    "    @property\n",
    "    @lru_cache(1)\n",
    "    def score_df(self):\n",
    "        if not self.score_path:\n",
    "            raise Exception(\"Please specify the score_path\")\n",
    "        return blocks.assemble(self.score_path)\n",
    "    \n",
    "    @property\n",
    "    def score_X(self):\n",
    "        return self.score_df[self.feature_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the ***Data*** session, yoda loads the config file and read the data from input_path. The data looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Data(**conf_dict['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sepal_length</th>\n      <th>sepal_width</th>\n      <th>petal_length</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>5.1</td>\n      <td>3.5</td>\n      <td>1.4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4.9</td>\n      <td>3.0</td>\n      <td>1.4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4.7</td>\n      <td>3.2</td>\n      <td>1.3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4.6</td>\n      <td>3.1</td>\n      <td>1.5</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "   sepal_length  sepal_width  petal_length\n0           0.0          1.0           2.0\n1           5.1          3.5           1.4\n2           4.9          3.0           1.4\n3           4.7          3.2           1.3\n4           4.6          3.1           1.5"
     },
     "metadata": {},
     "execution_count": 105
    }
   ],
   "source": [
    "data.X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0    51\n2    50\n1    50\nName: species, dtype: int64"
     },
     "metadata": {},
     "execution_count": 106
    }
   ],
   "source": [
    "data.y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, it will generate an object for the ***Train*** session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _import_from_string(classname: str):\n",
    "    components = classname.split('.')\n",
    "    mod = __import__(components[0])\n",
    "    for comp in components[1:]:\n",
    "        mod = getattr(mod, comp)\n",
    "    return mod\n",
    "\n",
    "class Train:\n",
    "    def __init__(self, estimator: str, params: dict):\n",
    "        self.estimator = _import_from_string(estimator)(**params)\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y: pd.Series, **kwargs):\n",
    "        self.estimator.fit(X, y, **kwags)\n",
    "    \n",
    "    def predict(self, X: pd.DataFrame, **kwargs):\n",
    "        self.estimator.predict(X, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = Train(**conf_dict['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.fit(data.X, data.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'split'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-116-de92eccfe4ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0meval_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconf_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"eval\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0meval_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconf_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"eval\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"metrics\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "#export\n",
    "def eval(estimator: sklearn.base.BaseEstimator = None, eval_X: pd.DataFrame = None, eval_y: pd.DataFrame = None, eval_type: str = None, metrics: str = None):\n",
    "    if eval_X is not None and eval_type is not None:\n",
    "        raise Exception(\"eval_path: (%s) and eval_type: (%s) cannot co-exist\" % (eval_path, eval_type))\n",
    "    \n",
    "    eval_res = dict()\n",
    "    for metric in metrics:\n",
    "        if eval_X:\n",
    "            estimator.fit(data.X, data.y)\n",
    "            pred_y = estimator.predict(eval_X)\n",
    "            avg, sd = SCORERS[\"metric\"](eval_y, pred_y)\n",
    "        \n",
    "        if eval_type:\n",
    "            scores = cross_val_score(estimator, data.X, data.y, scoring=\"metric\")\n",
    "        \n",
    "        eval_res[metric] = {\"sd\": sd, \"avg\": avg}\n",
    "    return eval_res\n",
    "\n",
    "\n",
    "if \"eval\" in conf_dict or data.eval_path:\n",
    "    eval_type = conf_dict.get(\"eval\")\n",
    "    eval_path = data.eval_path\n",
    "    conf_eval = conf_dict[\"eval\"].get(\"metrics\")\n",
    "    metrics = conf_eval.split(\",\") if conf_eval else None\n",
    "    eval(train.estimator, data.eval_X, data.eval_y, eval_type, metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run on GCP AI platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Before we run on AI platform, we need to create an image that have all depedencies installed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{shell}\n",
    "export PROJECT_ID=$(gcloud config list project --format \"value(core.project)\")\n",
    "export IMAGE_REPO_NAME=yoda\n",
    "export IMAGE_TAG=basic\n",
    "export IMAGE_URI=gcr.io/$PROJECT_ID/$IMAGE_REPO_NAME:$IMAGE_TAG\n",
    "\n",
    "docker build -f ../docker/Dockerfile.basic -t $IMAGE_URI ./\n",
    "docker push $IMAGE_URI\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('py37_2': conda)",
   "language": "python",
   "name": "python37664bitpy372condac21e17866107496d9f49ef54f61743ad"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}